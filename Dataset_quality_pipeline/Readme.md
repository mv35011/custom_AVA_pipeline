# Annotation Quality Assurance Pipeline

A comprehensive quality control system for multi-annotator spatio-temporal video annotation projects.

## ğŸ¯ What This Pipeline Does

This pipeline ensures high-quality annotations when multiple people are working on the same video dataset. It:
- Measures how well annotators agree with each other
- Identifies problematic annotations automatically
- Generates quality reports for research presentations
- Provides real-time monitoring of annotation quality

## ğŸ“ Quick Start

### Prerequisites
- Python 3.8+
- Your existing video processing pipeline (we don't handle video processing)
- Annotation files in CSV format

### Installation
```bash
git clone <your-repo>
cd annotation_quality_pipeline
pip install -r requirements.txt
```

### Basic Setup
```bash
# 1. Set up the pipeline structure
python scripts/setup_pipeline.py

# 2. Create annotation assignments (who annotates what)
python scripts/create_assignments.py --num-annotators 7 --overlap-ratio 0.2

# 3. Run quality checks
python scripts/run_quality_check.py

# 4. Generate reports
python scripts/generate_reports.py
```

## ğŸ”„ Workflow Overview

### Phase 1: Calibration (First Week)
```
Your Pipeline â†’ Extracts frames/annotations â†’ Golden Dataset (50 videos)
                                              â†“
All 7 annotators work on same 50 videos â†’ Quality Pipeline analyzes agreement
                                              â†“
Generate calibration report â†’ Update annotation guidelines
```

### Phase 2: Production with Overlap (Main Work)
```
Your Pipeline â†’ Produces annotation assignments
                â†“
Quality Pipeline â†’ Assigns 20% videos to 3 annotators (overlap)
                â†’ Assigns 80% videos to 1 annotator (unique)
                â†“
Annotators work â†’ Quality Pipeline monitors agreement
                â†“
Generate weekly quality reports
```

### Phase 3: Final Quality Check
```
All annotations complete â†’ Quality Pipeline final analysis
                        â†“
Generate conference-ready quality report
```

## ğŸ“Š Input Format

Your annotation CSV files should look like this:
```csv
video_name,timestamp,x1,y1,x2,y2,action_label,person_id
video_001,1.0,0.911,0.592,0.931,0.667,1,0
video_001,1.0,0.911,0.592,0.931,0.667,7,0
video_001,2.0,0.525,0.608,0.552,0.739,1,1
```

Place them in:
```
data/annotations/raw_annotations/
â”œâ”€â”€ annotator_1/
â”‚   â”œâ”€â”€ batch_001.csv
â”‚   â””â”€â”€ batch_002.csv
â”œâ”€â”€ annotator_2/
â”‚   â””â”€â”€ ...
```

## ğŸ¯ Key Quality Metrics

### 1. **Spatial IoU (Intersection over Union)**
- Measures how well bounding boxes overlap
- **Good**: > 0.8 (80% overlap)
- **Excellent**: > 0.9 (90% overlap)

### 2. **Temporal IoU** 
- Measures agreement on action timing
- **Good**: > 0.75 (75% time overlap)
- **Excellent**: > 0.85 (85% time overlap)

### 3. **Fleiss' Kappa**
- Measures agreement on action labels
- **Good**: > 0.7 (70% better than chance)
- **Excellent**: > 0.8 (80% better than chance)

## ğŸš¨ Quality Alerts

The pipeline automatically flags:
- **Low IoU samples**: Bounding boxes don't match well
- **Low Kappa samples**: Annotators disagree on action labels
- **Outlier annotators**: Someone consistently different from others
- **Problematic videos**: Certain videos cause lots of disagreement

## ğŸ“ˆ Monitoring Dashboard

Access real-time quality metrics:
```bash
python src/monitoring/dashboard.py
```

View in browser: `http://localhost:8050`

Dashboard shows:
- Live agreement scores per annotator
- Progress tracking
- Flagged samples requiring review
- Quality trends over time

## ğŸ“‹ Daily Workflow for Annotators

### For Annotators:
1. Check assignment file: `data/assignments/annotator_X_assignments.csv`
2. Annotate assigned videos using CVAT
3. Export annotations to: `data/annotations/raw_annotations/annotator_X/`
4. Check quality dashboard for feedback

### For Quality Manager (You):
1. Run daily quality check: `python scripts/run_quality_check.py`
2. Review flagged samples in dashboard
3. Update annotation guidelines if needed
4. Generate weekly reports: `python scripts/generate_reports.py`

## ğŸ“Š Reports Generated

### Daily Reports
- Agreement scores per annotator
- Flagged samples needing review
- Progress updates

### Weekly Reports  
- Quality trends analysis
- Annotator performance comparison
- Guideline update recommendations

### Final Conference Report
- Overall dataset quality metrics
- Statistical significance tests
- Comparison with benchmark datasets

## ğŸ› ï¸ Configuration

```yaml
quality_thresholds:
  spatial_iou: 0.8
  temporal_iou: 0.75
  fleiss_kappa: 0.7

overlap_settings:
  overlap_ratio: 0.2      # 20% of videos get multiple annotators
  annotators_per_overlap: 3

monitoring:
  dashboard_port: 8050
  alert_email: your-email@domain.com
```

## ğŸ”§ Troubleshooting

### Common Issues:

**"Low agreement scores"**
- Check if annotation guidelines are clear
- Run calibration session with problematic annotators
- Review flagged samples manually

**"Missing annotation files"**
- Ensure annotators export to correct directories
- Check file naming conventions
- Verify CSV format matches expected schema

**"Dashboard not loading"**
- Check if port 8050 is available
- Ensure all dependencies installed
- Check logs in `logs/dashboard.log`

## ğŸ“ For Quality report

Generate final quality report:
```bash
python scripts/generate_reports.py --final-report --include-statistics
```

This creates a comprehensive report with:
- Inter-annotator agreement statistics
- Quality validation methodology
- Comparison with standard benchmarks
- Confidence intervals and significance tests


## ğŸ”„ Integration with Your Existing Pipeline

This quality pipeline is designed to work **alongside** your existing video processing pipeline:



```
Your Pipeline:          Quality Pipeline:
Videos â†’ Frames    â†’    Annotations â†’ Quality Check
     â†“                           â†“
  Sampling           â†’    Agreement Analysis
     â†“                           â†“
 Annotation Setup   â†’    Reports & Monitoring
```

The quality pipeline only needs your annotation CSV files - it doesn't interfere with your video processing workflow!

### File structure 
```
annotation_quality_pipeline/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.yaml                 # Main configuration file
â”‚   â”œâ”€â”€ annotation_guidelines.md    # Detailed annotation rules
â”‚   â””â”€â”€ quality_thresholds.yaml     # IoU, Kappa thresholds
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_videos/                 # Original video files
â”‚   â”‚   â”œâ”€â”€ video_001.mp4
â”‚   â”‚   â”œâ”€â”€ video_002.mp4
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚
â”‚   â”œâ”€â”€ extracted_frames/           # Extracted frames per video
â”‚   â”‚   â”œâ”€â”€ video_001/
â”‚   â”‚   â”‚   â”œâ”€â”€ img_00001.jpg       # Frame 1
â”‚   â”‚   â”‚   â”œâ”€â”€ img_00031.jpg       # Frame 31 (1 sec later @ 30fps)
â”‚   â”‚   â”‚   â”œâ”€â”€ img_00061.jpg       # Frame 61 (2 sec later @ 30fps)
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ video_002/
â”‚   â”‚   â”‚   â”œâ”€â”€ img_00001.jpg
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚
â”‚   â”œâ”€â”€ annotations/                # All annotation files
â”‚   â”‚   â”œâ”€â”€ raw_annotations/        # Direct from CVAT/VIA
â”‚   â”‚   â”‚   â”œâ”€â”€ annotator_1/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ batch_001.csv
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ batch_002.csv
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”‚   â”œâ”€â”€ annotator_2/
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ processed_annotations/  # Standardized format
â”‚   â”‚   â”‚   â”œâ”€â”€ train.csv           # Final training annotations
â”‚   â”‚   â”‚   â”œâ”€â”€ val.csv             # Validation annotations
â”‚   â”‚   â”‚   â”œâ”€â”€ test.csv            # Test annotations
â”‚   â”‚   â”‚   â””â”€â”€ overlap_samples.csv # Overlap annotations for quality check
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ golden_dataset/         # Ground truth for calibration
â”‚   â”‚       â”œâ”€â”€ golden_train.csv
â”‚   â”‚       â””â”€â”€ golden_annotations_v1.csv
â”‚   â”‚
â”‚   â”œâ”€â”€ assignments/                # Who annotates what
â”‚   â”‚   â”œâ”€â”€ overlap_assignments.csv
â”‚   â”‚   â”œâ”€â”€ unique_assignments.csv
â”‚   â”‚   â””â”€â”€ annotator_workload.csv
â”‚   â”‚
â”‚   â””â”€â”€ quality_reports/            # Quality analysis results
â”‚       â”œâ”€â”€ daily_reports/
â”‚       â”œâ”€â”€ weekly_reports/
â”‚       â””â”€â”€ final_quality_report.pdf
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ data_preparation/           # OPTIONAL - Only if you need format conversion
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ format_converter.py     # Convert between frame_id and timestamps
â”‚   â”‚   â””â”€â”€ dataset_splitter.py     # Split into overlap/unique assignments
â”‚   â”‚
â”‚   â”œâ”€â”€ annotation_management/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ assignment_generator.py # Generate annotation assignments
â”‚   â”‚   â”œâ”€â”€ cvat_integration.py     # CVAT API integration
â”‚   â”‚   â”œâ”€â”€ annotation_validator.py # Basic format validation
â”‚   â”‚   â””â”€â”€ batch_processor.py      # Process annotation batches
â”‚   â”‚
â”‚   â”œâ”€â”€ quality_control/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ iou_calculator.py       # Calculate spatial IoU
â”‚   â”‚   â”œâ”€â”€ temporal_iou.py         # Calculate temporal IoU
â”‚   â”‚   â”œâ”€â”€ kappa_calculator.py     # Calculate Fleiss' Kappa
â”‚   â”‚   â”œâ”€â”€ agreement_analyzer.py   # Main quality analysis
â”‚   â”‚   â”œâ”€â”€ outlier_detector.py     # Detect problematic annotations
â”‚   â”‚   â””â”€â”€ quality_reporter.py     # Generate quality reports
â”‚   â”‚
â”‚   â”œâ”€â”€ monitoring/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ dashboard.py            # Real-time quality dashboard
â”‚   â”‚   â”œâ”€â”€ alert_system.py         # Quality threshold alerts
â”‚   â”‚   â”œâ”€â”€ progress_tracker.py     # Track annotation progress
â”‚   â”‚   â””â”€â”€ performance_metrics.py  # Annotator performance tracking
â”‚   â”‚
â”‚   â”œâ”€â”€ calibration/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ golden_dataset_creator.py # Create calibration dataset
â”‚   â”‚   â”œâ”€â”€ calibration_analyzer.py   # Analyze calibration results
â”‚   â”‚   â””â”€â”€ guideline_updater.py      # Update annotation guidelines
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ file_utils.py           # File I/O utilities
â”‚       â”œâ”€â”€ visualization.py        # Plot quality metrics
â”‚       â”œâ”€â”€ data_loader.py          # Load annotation data
â”‚       â””â”€â”€ logger.py               # Logging utilities
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup_pipeline.py           # Initial setup script
â”‚   â”œâ”€â”€ create_assignments.py       # Generate annotation assignments
â”‚   â”œâ”€â”€ run_quality_check.py        # Run quality analysis
â”‚   â”œâ”€â”€ generate_reports.py         # Generate quality reports
â”‚   â””â”€â”€ export_final_dataset.py     # Export final dataset
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_quality_metrics.py     # Test quality calculations
â”‚   â”œâ”€â”€ test_data_processing.py     # Test data processing
â”‚   â””â”€â”€ test_integration.py         # Integration tests
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ data_exploration.ipynb      # Explore annotation data
â”‚   â”œâ”€â”€ quality_analysis.ipynb      # Quality analysis experiments
â”‚   â””â”€â”€ visualization.ipynb         # Visualize quality metrics
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ setup_guide.md              # Setup instructions
â”‚   â”œâ”€â”€ annotation_workflow.md      # Annotation workflow
â”‚   â”œâ”€â”€ quality_pipeline_guide.md   # Quality pipeline usage
â”‚   â””â”€â”€ api_documentation.md        # API documentation
â”‚
â””â”€â”€ outputs/
    â”œâ”€â”€ final_dataset/              # Final processed dataset
    â”‚   â”œâ”€â”€ ava_format/             # AVA-compatible format
    â”‚   â”‚   â”œâ”€â”€ train.csv
    â”‚   â”‚   â”œâ”€â”€ val.csv
    â”‚   â”‚   â””â”€â”€ test.csv
    â”‚   â””â”€â”€ custom_format/          # Your custom format
    â”‚       â”œâ”€â”€ train.csv
    â”‚       â””â”€â”€ val.csv
    â”‚
    â”œâ”€â”€ quality_reports/            # Generated quality reports
    â””â”€â”€ visualizations/             # Quality metric plots
```